{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4d66c2-eb12-46fc-9368-8854afd5b59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28bb7b25-dc37-4d84-a1a3-ec9b0f78e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive feedback'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "# Load stopwords\n",
    "with open(\"../modle/corpora/stopwords/english\", 'r') as file:\n",
    "    sw = file.read().splitlines()\n",
    "\n",
    "# Load the model\n",
    "with open(\"../modle/modle.pickle\", \"rb\") as f:\n",
    "    modle = pickle.load(f)\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = pd.read_csv(\"../modle/vocabulary.txt\", header=None)\n",
    "tokens = vocab[0].tolist()\n",
    "\n",
    "# Initialize stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocessing(text):\n",
    "    data = pd.DataFrame([text], columns=[\"tweet\"])\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in x.split()))\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)\n",
    "    data[\"tweet\"] = data[\"tweet\"].replace(r'\\d+', ' ', regex=True)\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(ps.stem(x) for x in x.split()))\n",
    "    return data[\"tweet\"]\n",
    "\n",
    "# Vectorizer function\n",
    "def vectorizer(ds, tokens):\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(tokens)}\n",
    "    \n",
    "    vectorized_lst = []\n",
    "    for sentence in ds:\n",
    "        sentence_vector = np.zeros(len(tokens), dtype=np.float16)\n",
    "        for word in sentence.split():\n",
    "            if word in vocab_dict:\n",
    "                sentence_vector[vocab_dict[word]] = 1\n",
    "        vectorized_lst.append(sentence_vector)\n",
    "    \n",
    "    return np.array(vectorized_lst, dtype=np.float32)\n",
    "\n",
    "# Prediction function\n",
    "def get_prediction(vectorized_txt):\n",
    "    prediction = modle.predict(vectorized_txt)[0]  # Get the first prediction\n",
    "    if prediction == 1:\n",
    "        return \"negative feedback\"\n",
    "    else:\n",
    "        return \"positive feedback\"\n",
    "txt= \" I love it\"\n",
    "preprocessed_txt = preprocessing(txt)\n",
    "vectorized_txt = vectorizer(preprocessed_txt,tokens)\n",
    "prediction =get_prediction(vectorized_txt) \n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5558de13-89b7-454c-bf56-b21e610f8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation,'')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4ac4d7-d1fb-4e5d-b70b-78b659385ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open (\"../modle/corpora/stopwords/english\",'r') as file:\n",
    "    sw = file.read().splitlines()\n",
    "\n",
    "with open(\"../modle/modle.pickle\",\"rb\") as f:\n",
    "    modle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7f95b1-bb8d-4c65-b8bf-c7e26e65995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"../modle/vocabulary.txt\", header = None)\n",
    "tokens = vocab[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccd7a0e-14b6-4593-b05b-cf1cc2f2b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab97ac5-c394-4143-b637-f438d5e4642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    data = pd.DataFrame([text],columns=[\"tweet\"])\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x:\" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',x,flags =re.MULTILINE) for x in x.split()))\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)\n",
    "    data[\"tweet\"] = data[\"tweet\"].str.replace(r'\\d+', ' ', regex=True)\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x:\" \".join(x for x in x.split() if x not in sw))\n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x :\" \".join(ps.stem(x) for x in x.split()))\n",
    "    return data[\"tweet\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f2b5c97-6c2d-4711-b1e1-62c8479e9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"../modle/vocabulary.txt\", header = None)\n",
    "tokens = vocab[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a813b4-adbb-474d-81d1-bab2d07a94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(ds, vocabulary):\n",
    "    # Create a mapping of vocabulary for faster lookup\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    \n",
    "    vectorized_lst = []\n",
    "    for sentence in ds:\n",
    "        # Initialize a zero vector for the sentence\n",
    "        sentence_vector = np.zeros(len(vocabulary), dtype=np.float16)\n",
    "        \n",
    "        # Split the sentence into words and mark corresponding indices\n",
    "        for word in sentence.split():\n",
    "            if word in vocab_dict:\n",
    "                sentence_vector[vocab_dict[word]] = 1\n",
    "        \n",
    "        vectorized_lst.append(sentence_vector)\n",
    "    \n",
    "    # Convert the list of vectors to a NumPy array\n",
    "    vectorized_lst_new = np.array(vectorized_lst, dtype=np.float32)\n",
    "    \n",
    "    return vectorized_lst_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df763ec4-cb14-4e76-a5be-b77c296bcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vectorized_txt):\n",
    "    prediction = modle.predict(vectorized_txt)\n",
    "    if prediction == 1:\n",
    "        return \"negitive feedback\"\n",
    "    else:\n",
    "        return \"positive feedback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f06d81-0faf-476c-8b91-93696f659f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negitive feedback'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt= \"  i like it\"\n",
    "preprocessed_txt = preprocessing(txt)\n",
    "vectorized_txt = vectorizer(preprocessed_txt,tokens)\n",
    "prediction =get_prediction(vectorized_txt) \n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b295a1-85ea-4796-b448-72a3d4305973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6f3ea-d650-47b2-b52e-0594bf808246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
